#!/opt/conda/bin/python
# import external packages
from transformers import BertModel, AutoTokenizer, AutoModelForSeq2SeqLM
import transformers
import torch
import argparse

# import self-implemented
from src.network import Network
from src.utils import *


transformers.logging.set_verbosity(transformers.logging.CRITICAL)


parser = argparse.ArgumentParser()
parser.add_argument("file_path", type=str, help="Number of questions")
parser.add_argument("N_question", type=int, help="Number of questions")
args = parser.parse_args()
file_path = args.file_path
N_question = args.N_question

with open(file_path, "r") as f:
    context = f.read()

# initialize
# device = 'cuda' if torch.cuda.is_available() else 'cpu'
device = 'cpu'

tokenizer1 = AutoTokenizer.from_pretrained(
    "pretrained/tok1", local_files_only=True)
encoder1 = BertModel.from_pretrained("pretrained/model1", local_files_only=True)

tokenizer2 = AutoTokenizer.from_pretrained("pretrained/tok2", local_files_only=True)
QG_model = AutoModelForSeq2SeqLM.from_pretrained("pretrained/model2", local_files_only=True)

# model config
config = {
    'batch_size': 512,
    'dropout_rate': 0.5,
    'learning_rate': 1e-1,
    'epochs': 10
}


# Answer Extraction
model_path = './trained_model/answer_extractor.pth'
QE_model = Network(dropout=config['dropout_rate']).to(device)
QE_model.load_state_dict(torch.load(model_path)['model_state_dict'])
QE_model.eval()


pred_answer = predict_answer(QE_model, context, tokenizer1, encoder1, device)

# Question Generation based on the answer

questions = generate_questions(
    pred_answer, context, tokenizer2, QG_model, device, 256, N_question)

for question in questions:
    print(question[10:])