{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5s2YNbyUwmv"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOGUpsfVqNh0",
        "outputId": "018c7be1-67bc-4488-b47a-d5a2a6ff263d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.9/dist-packages (23.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-cache-dir transformers sentencepiece datasets torchsummaryX nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZoCbC_cqRJj",
        "outputId": "b3422007-c349-43e4-b81f-1a76693c6c3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.98)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.11.0)\n",
            "Requirement already satisfied: torchsummaryX in /usr/local/lib/python3.9/dist-packages (1.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from torchsummaryX) (2.0.0+cu118)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->torchsummaryX) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->torchsummaryX) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->torchsummaryX) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->torchsummaryX) (3.1.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->torchsummaryX) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->torchsummaryX) (16.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->torchsummaryX) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->torchsummaryX) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjyoB8rrI9e7",
        "outputId": "7ba4243a-6b0f-4fa3-f5fc-f35d58cced89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration, T5TokenizerFast\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import gc\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from collections import Counter\n",
        "from torchsummaryX import summary\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "import time\n",
        "#******************新：借助nltl标注词性****************\n",
        "import nltk\n",
        "nltk.download(\"book\")\n",
        "from nltk import word_tokenize\n",
        "#****************************************************\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUKwjrgJ2ElU",
        "outputId": "3cba8e8d-7314-4320-d7f8-a3508dcd1a65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#将当前位置锚定在google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkUqMq6uUdpA"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVhIFo2k46WF",
        "outputId": "27c67c04-1fec-48ab-a2bb-b4ecfd552005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 12, 128)\n"
          ]
        }
      ],
      "source": [
        "#paragraph encoder\n",
        "#初始化tokenizer（使得一句话分解成token并返回token_ID）\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\", do_lower_case=True)\n",
        "#初始化encoder（将token_ID根据上下文转化为向量）\n",
        "encoder1 = BertModel.from_pretrained(\"prajjwal1/bert-tiny\")\n",
        "\n",
        "#一个演示例子\n",
        "i = tokenizer1('This is a tokenization example, I love swimming', return_tensors=\"pt\")\n",
        "o = encoder1(**i, output_hidden_states=True)\n",
        "embedding = o.last_hidden_state.detach().numpy()\n",
        "#返回了12个token，每个token是一个长度为128的向量来表达其意\n",
        "print(embedding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264,
          "referenced_widgets": [
            "5390dd7760e04a81aa8ca1416e87c4aa",
            "e505ded6ced54b42b439a404a43e75d4",
            "f19274274da14b7280f6001464da70f4",
            "a23bb532a3f5441da1592ce5d356c459",
            "49e509e7eeed4325856f49a0a53e0404",
            "d88d864cf19846ca9f3b945913d0e0db",
            "d9d602e5cca7419888416929a970d598",
            "bbebe606b829463ba0d9f6e1d41c54c7",
            "031a5825bb784e898c3513c6113da26c",
            "d621877054eb4836aa16f34b2284d25e",
            "81f88c3e296e40dfb5f865b549c99299"
          ]
        },
        "id": "WMQMOfJFJb_K",
        "outputId": "63ad160b-6336-4ebc-dccc-cdc291b6f81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5390dd7760e04a81aa8ca1416e87c4aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 130319\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 11873\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#download training data\n",
        "datasets = load_dataset(\"squad_v2\")\n",
        "datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lgNbq71RU-91"
      },
      "outputs": [],
      "source": [
        "#这个函数按一定比例抽取上下文context中的答案和非答案部分，返回原文相应短语的embedding，和这个短语是答案（1）还是非答案（0）\n",
        "def extract_and_tokenize(context, answers, tokenizer, encoder):\n",
        "    embeddings, tokenized_context, _ = tokenize_and_encode_context(context, tokenizer, encoder)\n",
        "    binary_labels = generate_binary_labels(answers, tokenized_context, tokenizer)\n",
        "    embeddings, binary_labels = extract_answer_token_and_nonanswer_token(embeddings, binary_labels)\n",
        "    return embeddings, binary_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4aLSMLIcMP46"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_encode_context(context, tokenizer, encoder):\n",
        "    tokenized_context = []\n",
        "    embeddings = []\n",
        "    for text in context:\n",
        "        tokenization = tokenizer(text, return_tensors=\"pt\", max_length = 512, \n",
        "                                 truncation = True, padding='max_length')\n",
        "        context_token = tokenization['input_ids'][0].detach().numpy()\n",
        "        tokenized_context.append(context_token)\n",
        "        out = encoder(**tokenization, output_hidden_states=True)\n",
        "        embedding = out.last_hidden_state.detach().numpy()\n",
        "        assert(embedding.shape[1] == len(context_token))\n",
        "        embeddings.append(embedding[0])\n",
        "    return embeddings, tokenized_context, tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5B5XRc83RFi0"
      },
      "outputs": [],
      "source": [
        "def generate_binary_labels(answers, tokenized_context, tokenizer):\n",
        "    binary_labels = []\n",
        "    for k, answer in enumerate(answers):\n",
        "        answer_texts = answer['text']\n",
        "        context = tokenized_context[k]\n",
        "        binary_label = np.zeros(len(context))\n",
        "        for text in answer_texts:\n",
        "            tokenization = tokenizer(text, return_tensors=\"pt\")\n",
        "            answer_token = tokenization['input_ids'][0].detach().numpy()[1:-1]\n",
        "            answer_length = len(answer_token)\n",
        "            \n",
        "            for i in range(len(context) - answer_length + 1):\n",
        "                if np.all(context[i:i+answer_length] == answer_token):\n",
        "                    binary_label[i:i+answer_length] = 1\n",
        "        binary_labels.append(binary_label)\n",
        "\n",
        "    return binary_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dR_ehgsh1_WK"
      },
      "outputs": [],
      "source": [
        "def extract_answer_token_and_nonanswer_token(embeddings, binary_labels):\n",
        "    single_embeddings = []\n",
        "    single_labels = []\n",
        "    for i, embedding in enumerate(embeddings):\n",
        "        for k, token in enumerate(embedding):\n",
        "            if binary_labels[i][k] == 1:\n",
        "                single_embeddings.append(token)\n",
        "                single_labels.append(1)\n",
        "            else:\n",
        "                if np.random.randint(0, 100, 1)[0] == 0:\n",
        "                    single_embeddings.append(token)\n",
        "                    single_labels.append(0)\n",
        "      \n",
        "    return single_embeddings, single_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "U-8IrVyeI31T"
      },
      "outputs": [],
      "source": [
        "#Extract text-answers from training data\n",
        "train_dataset = datasets['train']\n",
        "dev_dataset = datasets['validation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UXR7DygYLa20"
      },
      "outputs": [],
      "source": [
        "#数据集过大，取前2000个样本训练\n",
        "partition = 2000\n",
        "train_context = train_dataset['context'][:partition]\n",
        "train_answers = train_dataset['answers'][:partition]\n",
        "dev_context = dev_dataset['context'][:partition//10]\n",
        "dev_answers = dev_dataset['answers'][:partition//10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OeEt8spBKViA"
      },
      "outputs": [],
      "source": [
        "train_x, train_y = extract_and_tokenize(train_context, train_answers, tokenizer1, encoder1)\n",
        "dev_x, dev_y = extract_and_tokenize(dev_context, dev_answers, tokenizer1, encoder1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdDSol8pOYGo",
        "outputId": "90a4055a-4fb8-4825-97d2-6b3721ed6325"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({1: 7708, 0: 10111})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#训练集中有10000个非答案短语和7700个答案短语\n",
        "Counter(train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aSz0y2foEGPx"
      },
      "outputs": [],
      "source": [
        "#模型超参\n",
        "config = {\n",
        "    'batch_size': 512,\n",
        "    'dropout_rate': 0.5,\n",
        "    'learning_rate': 1e-1,\n",
        "    'epochs': 10\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wTQpkhb6CfNh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2160475f-ce4d-44a6-e6e5-08e7b2b9e37c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-7809e68b0e07>:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  train_x = torch.Tensor(train_x)\n"
          ]
        }
      ],
      "source": [
        "#构建dataloader\n",
        "train_x = torch.Tensor(train_x) \n",
        "dev_x = torch.Tensor(dev_x) \n",
        "train_y = torch.Tensor(train_y)\n",
        "dev_y = torch.Tensor(dev_y) \n",
        "train_data = TensorDataset(train_x,train_y)\n",
        "dev_data = TensorDataset(dev_x,dev_y)\n",
        "train_dataloader = DataLoader(train_data, batch_size = config['batch_size'], \n",
        "                              shuffle = True, num_workers = 2) \n",
        "dev_dataloader = DataLoader(dev_data, batch_size = config['batch_size'], \n",
        "                              shuffle = True, num_workers = 2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YolVJe8jIpSM",
        "outputId": "9e8e4767-4c4e-4da4-ab70-e91cba34f8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 128])\n",
            "torch.Size([512])\n"
          ]
        }
      ],
      "source": [
        "for x, y in train_dataloader:\n",
        "    print(x.shape)\n",
        "    print(y.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZeprPSSUi4x"
      },
      "source": [
        "# Neural Answer Selector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "F92V8tO0Un6P"
      },
      "outputs": [],
      "source": [
        "#普通前馈网络结构\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        input_size = 128\n",
        "        output_size = 1\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            #layer 1\n",
        "            nn.Linear(input_size, input_size * 2),\n",
        "            nn.BatchNorm1d(input_size * 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            \n",
        "\n",
        "            #layer 2\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(input_size * 2, input_size * 4),\n",
        "            nn.BatchNorm1d(input_size * 4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            #layer 3\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(input_size * 4, input_size * 8),\n",
        "            nn.BatchNorm1d(input_size * 8),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "\n",
        "            #layer 4\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(input_size * 8, input_size * 4),\n",
        "            nn.BatchNorm1d(input_size * 4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "\n",
        "            #layer 5\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(input_size * 4, input_size * 2),\n",
        "            nn.BatchNorm1d(input_size * 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            #layer 6\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(input_size * 2, input_size), \n",
        "            nn.BatchNorm1d(input_size),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            #layer 7\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(input_size, input_size // 2), \n",
        "            nn.BatchNorm1d(input_size // 2),\n",
        "            nn.LeakyReLU(0.21),\n",
        "            \n",
        "            #layer 8\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(input_size // 2, input_size // 4),\n",
        "            nn.BatchNorm1d(input_size // 4),   \n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            #layer 9\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(input_size // 4, input_size // 8), \n",
        "            nn.BatchNorm1d(input_size // 8),  \n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            #layer 10\n",
        "            nn.Linear(input_size // 8, output_size),  \n",
        "            nn.Sigmoid(),\n",
        "        )      \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W55n4PlqDNFW",
        "outputId": "a859a75e-7e1e-4ea1-ab17-18f505b4da8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "                        Kernel Shape Output Shape    Params Mult-Adds\n",
            "Layer                                                                \n",
            "0_model.Linear_0          [128, 256]   [512, 256]   33.024k   32.768k\n",
            "1_model.BatchNorm1d_1          [256]   [512, 256]     512.0     256.0\n",
            "2_model.LeakyReLU_2                -   [512, 256]         -         -\n",
            "3_model.Dropout_3                  -   [512, 256]         -         -\n",
            "4_model.Linear_4          [256, 512]   [512, 512]  131.584k  131.072k\n",
            "5_model.BatchNorm1d_5          [512]   [512, 512]    1.024k     512.0\n",
            "6_model.LeakyReLU_6                -   [512, 512]         -         -\n",
            "7_model.Dropout_7                  -   [512, 512]         -         -\n",
            "8_model.Linear_8         [512, 1024]  [512, 1024]  525.312k  524.288k\n",
            "9_model.BatchNorm1d_9         [1024]  [512, 1024]    2.048k    1.024k\n",
            "10_model.LeakyReLU_10              -  [512, 1024]         -         -\n",
            "11_model.Dropout_11                -  [512, 1024]         -         -\n",
            "12_model.Linear_12       [1024, 512]   [512, 512]    524.8k  524.288k\n",
            "13_model.BatchNorm1d_13        [512]   [512, 512]    1.024k     512.0\n",
            "14_model.LeakyReLU_14              -   [512, 512]         -         -\n",
            "15_model.Dropout_15                -   [512, 512]         -         -\n",
            "16_model.Linear_16        [512, 256]   [512, 256]  131.328k  131.072k\n",
            "17_model.BatchNorm1d_17        [256]   [512, 256]     512.0     256.0\n",
            "18_model.LeakyReLU_18              -   [512, 256]         -         -\n",
            "19_model.Dropout_19                -   [512, 256]         -         -\n",
            "20_model.Linear_20        [256, 128]   [512, 128]   32.896k   32.768k\n",
            "21_model.BatchNorm1d_21        [128]   [512, 128]     256.0     128.0\n",
            "22_model.LeakyReLU_22              -   [512, 128]         -         -\n",
            "23_model.Dropout_23                -   [512, 128]         -         -\n",
            "24_model.Linear_24         [128, 64]    [512, 64]    8.256k    8.192k\n",
            "25_model.BatchNorm1d_25         [64]    [512, 64]     128.0      64.0\n",
            "26_model.LeakyReLU_26              -    [512, 64]         -         -\n",
            "27_model.Dropout_27                -    [512, 64]         -         -\n",
            "28_model.Linear_28          [64, 32]    [512, 32]     2.08k    2.048k\n",
            "29_model.BatchNorm1d_29         [32]    [512, 32]      64.0      32.0\n",
            "30_model.LeakyReLU_30              -    [512, 32]         -         -\n",
            "31_model.Dropout_31                -    [512, 32]         -         -\n",
            "32_model.Linear_32          [32, 16]    [512, 16]     528.0     512.0\n",
            "33_model.BatchNorm1d_33         [16]    [512, 16]      32.0      16.0\n",
            "34_model.LeakyReLU_34              -    [512, 16]         -         -\n",
            "35_model.Linear_35           [16, 1]     [512, 1]      17.0      16.0\n",
            "36_model.Sigmoid_36                -     [512, 1]         -         -\n",
            "----------------------------------------------------------------------\n",
            "                         Totals\n",
            "Total params          1.395425M\n",
            "Trainable params      1.395425M\n",
            "Non-trainable params        0.0\n",
            "Mult-Adds             1.389824M\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: The default value of numeric_only in DataFrame.sum is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        Kernel Shape Output Shape    Params  Mult-Adds\n",
              "Layer                                                                 \n",
              "0_model.Linear_0          [128, 256]   [512, 256]   33024.0    32768.0\n",
              "1_model.BatchNorm1d_1          [256]   [512, 256]     512.0      256.0\n",
              "2_model.LeakyReLU_2                -   [512, 256]       NaN        NaN\n",
              "3_model.Dropout_3                  -   [512, 256]       NaN        NaN\n",
              "4_model.Linear_4          [256, 512]   [512, 512]  131584.0   131072.0\n",
              "5_model.BatchNorm1d_5          [512]   [512, 512]    1024.0      512.0\n",
              "6_model.LeakyReLU_6                -   [512, 512]       NaN        NaN\n",
              "7_model.Dropout_7                  -   [512, 512]       NaN        NaN\n",
              "8_model.Linear_8         [512, 1024]  [512, 1024]  525312.0   524288.0\n",
              "9_model.BatchNorm1d_9         [1024]  [512, 1024]    2048.0     1024.0\n",
              "10_model.LeakyReLU_10              -  [512, 1024]       NaN        NaN\n",
              "11_model.Dropout_11                -  [512, 1024]       NaN        NaN\n",
              "12_model.Linear_12       [1024, 512]   [512, 512]  524800.0   524288.0\n",
              "13_model.BatchNorm1d_13        [512]   [512, 512]    1024.0      512.0\n",
              "14_model.LeakyReLU_14              -   [512, 512]       NaN        NaN\n",
              "15_model.Dropout_15                -   [512, 512]       NaN        NaN\n",
              "16_model.Linear_16        [512, 256]   [512, 256]  131328.0   131072.0\n",
              "17_model.BatchNorm1d_17        [256]   [512, 256]     512.0      256.0\n",
              "18_model.LeakyReLU_18              -   [512, 256]       NaN        NaN\n",
              "19_model.Dropout_19                -   [512, 256]       NaN        NaN\n",
              "20_model.Linear_20        [256, 128]   [512, 128]   32896.0    32768.0\n",
              "21_model.BatchNorm1d_21        [128]   [512, 128]     256.0      128.0\n",
              "22_model.LeakyReLU_22              -   [512, 128]       NaN        NaN\n",
              "23_model.Dropout_23                -   [512, 128]       NaN        NaN\n",
              "24_model.Linear_24         [128, 64]    [512, 64]    8256.0     8192.0\n",
              "25_model.BatchNorm1d_25         [64]    [512, 64]     128.0       64.0\n",
              "26_model.LeakyReLU_26              -    [512, 64]       NaN        NaN\n",
              "27_model.Dropout_27                -    [512, 64]       NaN        NaN\n",
              "28_model.Linear_28          [64, 32]    [512, 32]    2080.0     2048.0\n",
              "29_model.BatchNorm1d_29         [32]    [512, 32]      64.0       32.0\n",
              "30_model.LeakyReLU_30              -    [512, 32]       NaN        NaN\n",
              "31_model.Dropout_31                -    [512, 32]       NaN        NaN\n",
              "32_model.Linear_32          [32, 16]    [512, 16]     528.0      512.0\n",
              "33_model.BatchNorm1d_33         [16]    [512, 16]      32.0       16.0\n",
              "34_model.LeakyReLU_34              -    [512, 16]       NaN        NaN\n",
              "35_model.Linear_35           [16, 1]     [512, 1]      17.0       16.0\n",
              "36_model.Sigmoid_36                -     [512, 1]       NaN        NaN"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-51400506-18a5-4c2b-8fc5-8c3861449fa4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_model.Linear_0</th>\n",
              "      <td>[128, 256]</td>\n",
              "      <td>[512, 256]</td>\n",
              "      <td>33024.0</td>\n",
              "      <td>32768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_model.BatchNorm1d_1</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[512, 256]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_model.LeakyReLU_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_model.Dropout_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_model.Linear_4</th>\n",
              "      <td>[256, 512]</td>\n",
              "      <td>[512, 512]</td>\n",
              "      <td>131584.0</td>\n",
              "      <td>131072.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_model.BatchNorm1d_5</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[512, 512]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_model.LeakyReLU_6</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_model.Dropout_7</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_model.Linear_8</th>\n",
              "      <td>[512, 1024]</td>\n",
              "      <td>[512, 1024]</td>\n",
              "      <td>525312.0</td>\n",
              "      <td>524288.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_model.BatchNorm1d_9</th>\n",
              "      <td>[1024]</td>\n",
              "      <td>[512, 1024]</td>\n",
              "      <td>2048.0</td>\n",
              "      <td>1024.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_model.LeakyReLU_10</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_model.Dropout_11</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_model.Linear_12</th>\n",
              "      <td>[1024, 512]</td>\n",
              "      <td>[512, 512]</td>\n",
              "      <td>524800.0</td>\n",
              "      <td>524288.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_model.BatchNorm1d_13</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[512, 512]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_model.LeakyReLU_14</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15_model.Dropout_15</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16_model.Linear_16</th>\n",
              "      <td>[512, 256]</td>\n",
              "      <td>[512, 256]</td>\n",
              "      <td>131328.0</td>\n",
              "      <td>131072.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_model.BatchNorm1d_17</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[512, 256]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18_model.LeakyReLU_18</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19_model.Dropout_19</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_model.Linear_20</th>\n",
              "      <td>[256, 128]</td>\n",
              "      <td>[512, 128]</td>\n",
              "      <td>32896.0</td>\n",
              "      <td>32768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21_model.BatchNorm1d_21</th>\n",
              "      <td>[128]</td>\n",
              "      <td>[512, 128]</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22_model.LeakyReLU_22</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23_model.Dropout_23</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24_model.Linear_24</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>[512, 64]</td>\n",
              "      <td>8256.0</td>\n",
              "      <td>8192.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25_model.BatchNorm1d_25</th>\n",
              "      <td>[64]</td>\n",
              "      <td>[512, 64]</td>\n",
              "      <td>128.0</td>\n",
              "      <td>64.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26_model.LeakyReLU_26</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 64]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27_model.Dropout_27</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 64]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28_model.Linear_28</th>\n",
              "      <td>[64, 32]</td>\n",
              "      <td>[512, 32]</td>\n",
              "      <td>2080.0</td>\n",
              "      <td>2048.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29_model.BatchNorm1d_29</th>\n",
              "      <td>[32]</td>\n",
              "      <td>[512, 32]</td>\n",
              "      <td>64.0</td>\n",
              "      <td>32.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30_model.LeakyReLU_30</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 32]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31_model.Dropout_31</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 32]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32_model.Linear_32</th>\n",
              "      <td>[32, 16]</td>\n",
              "      <td>[512, 16]</td>\n",
              "      <td>528.0</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33_model.BatchNorm1d_33</th>\n",
              "      <td>[16]</td>\n",
              "      <td>[512, 16]</td>\n",
              "      <td>32.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34_model.LeakyReLU_34</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 16]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35_model.Linear_35</th>\n",
              "      <td>[16, 1]</td>\n",
              "      <td>[512, 1]</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36_model.Sigmoid_36</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 1]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51400506-18a5-4c2b-8fc5-8c3861449fa4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-51400506-18a5-4c2b-8fc5-8c3861449fa4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-51400506-18a5-4c2b-8fc5-8c3861449fa4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "model = Network(dropout = config['dropout_rate']).to(device)\n",
        "token, label = next(iter(train_dataloader))\n",
        "summary(model, token.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OHi_M17tJRxS"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss() #Defining Loss function \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'max', factor = 0.7, patience = 5, verbose = True, min_lr = 1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqDCkO0OMj2G",
        "outputId": "d641276a-4dd2-4026-ec91-55d4c2bf4a20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MEN77PHvMrRA"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, dataloader):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0.0 #Monitoring Loss\n",
        "    label_true_list = []\n",
        "    label_pred_list = []\n",
        "    \n",
        "    for iter, (tokens, labels) in enumerate(tqdm(dataloader)):\n",
        "\n",
        "        ### Move Data to Device (Ideally GPU)\n",
        "        tokens = tokens.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        ### Forward Propagation\n",
        "        predictions = model(tokens)\n",
        "\n",
        "        ### Loss Calculation\n",
        "        loss = criterion(predictions.reshape(-1), labels)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        ### Initialize Gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        ### Backward Propagation\n",
        "        loss.backward()\n",
        "\n",
        "        ### Gradient Descent\n",
        "        optimizer.step()\n",
        "\n",
        "        predictions = predictions.detach().numpy().reshape(-1)\n",
        "        predictions[predictions >= 0.5] = 1\n",
        "        predictions[predictions < 0.5] = 0\n",
        "\n",
        "        ### Store Pred and True Labels\n",
        "        label_pred_list.extend(list(predictions))\n",
        "        label_true_list.extend(labels.detach().tolist())\n",
        "\n",
        "        del tokens, labels, predictions\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    train_loss /= len(dataloader)\n",
        "    accuracy = accuracy_score(label_true_list, label_pred_list) \n",
        "    return train_loss, accuracy * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_OpmQQmjNrui"
      },
      "outputs": [],
      "source": [
        "def eval(model, dataloader):\n",
        "\n",
        "    model.eval() # set model in evaluation mode\n",
        "\n",
        "    label_true_list = []\n",
        "    label_pred_list = []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(tqdm(dataloader)):\n",
        "\n",
        "        tokens, labels = data\n",
        "        ### Move data to device (ideally GPU)\n",
        "        tokens, labels = tokens.to(device), labels.to(device) \n",
        "\n",
        "        with torch.inference_mode(): # makes sure that there are no gradients computed as we are not training the model now\n",
        "            ### Forward Propagation\n",
        "            predictions = model(tokens)\n",
        "            loss = criterion(predictions.reshape(-1), labels)\n",
        "            total_loss += float(loss.item())\n",
        "      \n",
        "        predictions = predictions.cpu().numpy().reshape(-1)\n",
        "        predictions[predictions >= 0.5] = 1\n",
        "        predictions[predictions < 0.5] = 0\n",
        "        \n",
        "        \n",
        "        ### Store Pred and True Labels\n",
        "        label_pred_list.extend(list(predictions))\n",
        "        label_true_list.extend(labels.cpu().tolist())\n",
        "    \n",
        "        del tokens, labels, predictions\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    ### Calculate Accuracy\n",
        "    accuracy = accuracy_score(label_true_list, label_pred_list) \n",
        "    total_loss = float(total_loss / len(dataloader))\n",
        "    return total_loss, accuracy * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment: Training"
      ],
      "metadata": {
        "id": "EnZm4EXowa3k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "azb3JnHYO-ps"
      },
      "outputs": [],
      "source": [
        "# #模型已经训练好了\n",
        "# # Iterate over number of epochs to train and evaluate your model\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# best_accuracy = 0\n",
        "\n",
        "# for epoch in range(config['epochs']):\n",
        "#     print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "#     train_loss, train_accuracy = train(model, optimizer, criterion, train_dataloader)\n",
        "#     val_loss, val_accuracy = eval(model, dev_dataloader)\n",
        "\n",
        "#     scheduler.step(val_loss)\n",
        "\n",
        "#     print(\"Train Loss: {:.4f}\".format(train_loss))\n",
        "#     print(\"Training Accuracy: {:.2f}%\".format(train_accuracy))\n",
        "#     print(\"Validation Loss: {:.4f}\".format(val_loss))\n",
        "#     print(\"Validation Accuracy: {:.2f}%\".format(val_accuracy))\n",
        "\n",
        "\n",
        "#     if val_accuracy > best_accuracy:\n",
        "#         best_accuracy = val_accuracy\n",
        "#         print(\"Saving model\")\n",
        "#         torch.save({'model_state_dict':model.state_dict(),\n",
        "#                     'optimizer_state_dict':optimizer.state_dict(),\n",
        "#                     'val_accuracy': val_accuracy, \n",
        "#                     'epoch': epoch}, '/content/drive/MyDrive/11611/Project/answer_extractor.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHnaCGHi2xq9"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrZoUJ9021qi",
        "outputId": "05218d00-15ed-4cea-f8a1-b1af6816c21b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (model): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
              "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): LeakyReLU(negative_slope=0.2)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): LeakyReLU(negative_slope=0.2)\n",
              "    (7): Dropout(p=0.5, inplace=False)\n",
              "    (8): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    (9): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): LeakyReLU(negative_slope=0.2)\n",
              "    (11): Dropout(p=0.5, inplace=False)\n",
              "    (12): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (14): LeakyReLU(negative_slope=0.2)\n",
              "    (15): Dropout(p=0.5, inplace=False)\n",
              "    (16): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (17): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (18): LeakyReLU(negative_slope=0.2)\n",
              "    (19): Dropout(p=0.5, inplace=False)\n",
              "    (20): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (21): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (22): LeakyReLU(negative_slope=0.2)\n",
              "    (23): Dropout(p=0.5, inplace=False)\n",
              "    (24): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (25): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (26): LeakyReLU(negative_slope=0.21)\n",
              "    (27): Dropout(p=0.5, inplace=False)\n",
              "    (28): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (29): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (30): LeakyReLU(negative_slope=0.2)\n",
              "    (31): Dropout(p=0.5, inplace=False)\n",
              "    (32): Linear(in_features=32, out_features=16, bias=True)\n",
              "    (33): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (34): LeakyReLU(negative_slope=0.2)\n",
              "    (35): Linear(in_features=16, out_features=1, bias=True)\n",
              "    (36): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "#Load trained model\n",
        "#模型已经训练好了， 载入模型\n",
        "PATH = '/content/drive/MyDrive/11611/Project/answer_extractor.pth'\n",
        "QE_model = Network(dropout = config['dropout_rate']).to('cpu')\n",
        "QE_model.load_state_dict(torch.load(PATH)['model_state_dict'])\n",
        "QE_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ylR8sCbc4MLq"
      },
      "outputs": [],
      "source": [
        "#以下系列函数都是用来把模型预测属于答案的embeddings对应回原文相应的短语，返回一个包含答案短语字符串的[]\n",
        "def predict_answer(model, context, tokenizer, encoder):\n",
        "    words_list, words_ids_list, words_mapping = context_to_words_inputids(context, tokenizer)\n",
        "    embeddings, _, tokenization = tokenize_and_encode_context([context], tokenizer, encoder)\n",
        "    predictions = model_predict(model, embeddings)\n",
        "    input_ids = tokenization['input_ids'][0].detach().numpy().tolist()\n",
        "    answer_ids_list = get_answer_ids(input_ids, predictions)\n",
        "    answers = map_answer_ids_to_words(answer_ids_list, words_ids_list, words_mapping, words_list)\n",
        "    answers = sorted(set(answers), key=answers.index)\n",
        "    sentences = split_into_sentences(context)\n",
        "    answers_sentences = align_answers_with_sentences(answers, sentences)\n",
        "    return answers_sentences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def align_answers_with_sentences(answers, sentences):\n",
        "    answers_sentences = []\n",
        "    for answer in answers:\n",
        "        for sentence in sentences:\n",
        "            if answer in sentence:\n",
        "                answers_sentences.append((answer, sentence))\n",
        "                break\n",
        "    return answers_sentences"
      ],
      "metadata": {
        "id": "OebsDs_c-GX3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alphabets= \"([A-Za-z])\"\n",
        "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
        "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
        "starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
        "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
        "websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
        "digits = \"([0-9])\"\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    text = \" \" + text + \"  \"\n",
        "    text = text.replace(\"\\n\",\" \")\n",
        "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
        "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
        "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
        "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
        "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
        "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
        "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
        "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
        "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
        "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
        "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
        "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
        "    text = text.replace(\".\",\".<stop>\")\n",
        "    text = text.replace(\"?\",\"?<stop>\")\n",
        "    text = text.replace(\"!\",\"!<stop>\")\n",
        "    text = text.replace(\"<prd>\",\".\")\n",
        "    sentences = text.split(\"<stop>\")\n",
        "    sentences = sentences[:-1]\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "IqPcA3ko9jXx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "gaekNBIOqhrv"
      },
      "outputs": [],
      "source": [
        "def map_answer_ids_to_words(answer_ids_list, words_ids_list, words_mapping, words_list):\n",
        "    text_answer_list = []\n",
        "    for ans in answer_ids_list:\n",
        "        ans_length = len(ans)\n",
        "        for i in range(len(words_ids_list) - ans_length + 1):\n",
        "            if words_ids_list[i:i+len(ans)] == ans:\n",
        "                corresponding_words = words_mapping[i:i+len(ans)]\n",
        "                corresponding_words = sorted(set(corresponding_words), key=corresponding_words.index)\n",
        "                text_ans = \"\"\n",
        "                for w in corresponding_words:\n",
        "                    text_ans += ' ' + words_list[w]\n",
        "                text_answer_list.append(text_ans[1:])\n",
        "    return text_answer_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "A7bcFo-Jo7mG"
      },
      "outputs": [],
      "source": [
        "def model_predict(model, embeddings):\n",
        "    embeddings = torch.tensor(embeddings[0])\n",
        "    predictions = model(embeddings).flatten()\n",
        "    predictions[predictions >= 0.5] = 1\n",
        "    predictions[predictions < 0.5] = 0\n",
        "    predictions = predictions.detach().numpy().tolist()\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "n-opS3BemtkI"
      },
      "outputs": [],
      "source": [
        "def get_answer_ids(input_ids, predictions):\n",
        "    assert(len(input_ids) == len(predictions))\n",
        "    all_answers_ids = []\n",
        "    \n",
        "    i = 0\n",
        "    while i < len(predictions) - 1:\n",
        "        if predictions[i] == 1:\n",
        "            ans_span = [input_ids[i]]\n",
        "            j = i + 1\n",
        "            while j < len(predictions):\n",
        "                if predictions[j] == 1:\n",
        "                    ans_span.append(input_ids[j])\n",
        "                    j += 1\n",
        "                else:\n",
        "                    all_answers_ids.append(ans_span)\n",
        "                    i = j\n",
        "                    break\n",
        "        else:\n",
        "            i += 1\n",
        "            \n",
        "    return all_answers_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "dyoBcizIkJ51"
      },
      "outputs": [],
      "source": [
        "def context_to_words_inputids(context,  tokenizer):\n",
        "    word_ids_list = []\n",
        "    word_mapping = []\n",
        "    words_list = context_to_words(context)\n",
        "    for k, word in enumerate(words_list):\n",
        "        word_id = tokenizer(word)['input_ids'][1:-1]\n",
        "        word_ids_list.extend(word_id)\n",
        "        word_mapping.extend([k for i in range(len(word_id))])\n",
        "    return words_list, word_ids_list, word_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "RtqItozcGmj9"
      },
      "outputs": [],
      "source": [
        "def context_to_words(context):\n",
        "    return re.sub(\"[^\\w]\", \" \",  context).split()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#since in the dataset a lot of context is same, we remove duplicates\n",
        "#测试：去除测试集中的重复文章（测试集都是一个文章对好几个答案）\n",
        "dev_context_no_dup = list(set(dev_context))"
      ],
      "metadata": {
        "id": "en1GaAGpw3M4"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeM45xbMzuf9",
        "outputId": "bfd40b69-13a1-4eef-e541-872ab5ceec7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 38/38 [00:02<00:00, 12.98it/s]\n"
          ]
        }
      ],
      "source": [
        "#对每篇测试文章都生生成对应的若干答案\n",
        "pred_answers = []\n",
        "for cont in tqdm(dev_context_no_dup):\n",
        "    pred_answer = predict_answer(QE_model, cont, tokenizer1, encoder1)\n",
        "    pred_answers.append(pred_answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"/content/drive/MyDrive/11611/Project/context.txt\", \"w\") as f:\n",
        "#     for cont in dev_context_no_dup:\n",
        "#         f.write(cont)\n",
        "#         f.write('\\n')"
      ],
      "metadata": {
        "id": "7c_q5bHAx1jP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"/content/drive/MyDrive/11611/Project/potential_answers.txt\", \"w\") as f:\n",
        "#     for answers in pred_answers:\n",
        "#         n = len(answers)\n",
        "#         for i, ans in enumerate(answers):\n",
        "#             f.write(ans)\n",
        "#             if i == n - 1:\n",
        "#                 continue\n",
        "#             f.write(',')\n",
        "#         f.write('\\n')"
      ],
      "metadata": {
        "id": "fjOxw8kWz2kE"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Questions "
      ],
      "metadata": {
        "id": "H_CDErHhTOdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#******************新：MixQG model****************\n",
        "MixQG_tokenizer = AutoTokenizer.from_pretrained('Salesforce/mixqg-base')\n",
        "MixQG_model = AutoModelForSeq2SeqLM.from_pretrained('Salesforce/mixqg-base')\n",
        "#******************新：Answer-agnostic Model****************\n",
        "answer_agnositic_model = T5ForConditionalGeneration.from_pretrained(\"ThomasSimonini/t5-end2end-question-generation\")\n",
        "answer_agnositic_tokenizer = T5TokenizerFast.from_pretrained('t5-small')"
      ],
      "metadata": {
        "id": "9ZT1dWUppd7r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "731a3565-f5f1-4e82-96d0-c1d59c5c808e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Get_Questions_Main(context, N = 10, question_type = 'any', question_content = 'any'):\n",
        "    if question_type not in [\"any\", \"Wh-question\", \"TF-question\"]:\n",
        "        print(\"Question type should be in one of the following: Wh-question, TF-question or any!\")\n",
        "        return None\n",
        "    if question_content not in [\"any\", \"entity\", \"event\", \"number\"]:\n",
        "        print(\"Question content should be in one of the following: entity, event, number or any!\")\n",
        "        return None\n",
        "    if question_type == 'any' and question_content == 'any':\n",
        "        T1 = time.time()\n",
        "        answers_part1 = run_answer_agnostic_model(context, N)\n",
        "        T2 = time.time()\n",
        "        print(\"Answer agnostic model consumes: \", np.round(T2 - T1, 3), \" seconds!\")\n",
        "        answers_part2 = run_answer_aware_model(context, N = N - len(answers_part1), question_type = 'any', question_content = 'any')\n",
        "        T3 = time.time()\n",
        "        print(\"Answer aware model consumes: \", np.round(T3 - T2, 3),\" seconds!\")\n",
        "        answers = answers_part1 + answers_part2\n",
        "    else:\n",
        "        answers = run_answer_aware_model(context, N = N, question_type = question_type, question_content = question_content)\n",
        "    print(\"\\n\\n****************************Questions****************************\")\n",
        "    for q in answers:\n",
        "        print(q)\n",
        "    return answers"
      ],
      "metadata": {
        "id": "--4dhzBkncp9"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_answer_agnostic_model(context, N):\n",
        "    #将原文裁剪到只剩N句话\n",
        "    context_sentences = split_into_sentences(context)\n",
        "    context_sentences = list(np.random.choice(context_sentences, size=N, replace=False))\n",
        "    context_short = ' '.join(context_sentences)\n",
        "\n",
        "    generator_args = {\n",
        "    \"max_length\": 2048,\n",
        "    \"num_beams\": 4,\n",
        "    \"length_penalty\": 1.5,\n",
        "    \"no_repeat_ngram_size\": 10,\n",
        "    \"early_stopping\": True,\n",
        "    }\n",
        "    context = \"generate questions: \" + context_short + \" </s>\"\n",
        "    input_ids = answer_agnositic_tokenizer.encode(context, return_tensors=\"pt\")\n",
        "    res = answer_agnositic_model.generate(input_ids, **generator_args)\n",
        "    output = answer_agnositic_tokenizer.batch_decode(res, skip_special_tokens=True)\n",
        "    output = [item.split(\"<sep>\") for item in output]\n",
        "    output = answer_agnostic_model_postprocess_output(output)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "lwRjiLZ4nE-7"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_agnostic_model_postprocess_output(output):\n",
        "    output = output[0][0]\n",
        "    if output[-1] != '?':\n",
        "        output = output + '?'\n",
        "    output = output.split('? ')\n",
        "    for i in range(len(output)):\n",
        "        if output[i][-1] != '?':\n",
        "            output[i] = output[i] + '?'\n",
        "    return output"
      ],
      "metadata": {
        "id": "SzAzv-CEnSTV"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_answer_aware_model(context, N = 10, question_type = 'any', question_content = 'any'):\n",
        "    answers_sentences = predict_answer(QE_model, context, tokenizer1, encoder1)\n",
        "    answers_sentences = catagorize_answers(answers_sentences, question_content)\n",
        "    if len(answers_sentences) == 0:\n",
        "        print(\"Cannot generate required questions!\")\n",
        "        return None\n",
        "    model_input = format_type_inputs(answers_sentences, question_type)\n",
        "    model_input = sample_input(model_input, N)\n",
        "    pred_quesitons = get_question(model_input)\n",
        "    return pred_quesitons[:N]"
      ],
      "metadata": {
        "id": "Ovg79q1gVxP1"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def catagorize_answers(answers_sentences, question_content):\n",
        "    cate_answers = dict()\n",
        "    cate_answers['entity'] = []\n",
        "    cate_answers['event'] = []\n",
        "    cate_answers['number'] = []\n",
        "    cate_answers['any'] = []\n",
        "\n",
        "    for pair in answers_sentences:\n",
        "        answer = pair[0]\n",
        "        sentence = pair[1]\n",
        "        tokens = word_tokenize(answer)\n",
        "        tags = nltk.pos_tag(tokens, tagset = \"universal\")\n",
        "\n",
        "        NOUN = False\n",
        "        VERB = False\n",
        "        NUM = None\n",
        "        NUM_sentence = None\n",
        "\n",
        "        for tag in tags:\n",
        "            word = tag[0]\n",
        "            POS = tag[1]\n",
        "\n",
        "            if POS == 'NOUN':\n",
        "                NOUN = True\n",
        "            if POS == 'VERB':\n",
        "                VERB = True\n",
        "            if POS == 'NUM':\n",
        "                NUM = word\n",
        "                NUM_sentence = sentence\n",
        "          \n",
        "        if NOUN or VERB or (NUM != None):\n",
        "            cate_answers['any'].append(pair)\n",
        "        \n",
        "        if VERB:\n",
        "            cate_answers['event'].append(pair)\n",
        "        elif NUM != None:\n",
        "            cate_answers['number'].append((NUM, sentence))\n",
        "        elif NOUN:\n",
        "            cate_answers['entity'].append(pair)\n",
        "            \n",
        "    return cate_answers[question_content]\n"
      ],
      "metadata": {
        "id": "yEaa2RXZXXNS"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_input(model_input, N):\n",
        "    if len(model_input) > N:\n",
        "        return list(np.random.choice(model_input, size = N, replace = False))\n",
        "    else:\n",
        "        return model_input"
      ],
      "metadata": {
        "id": "OkF0L6NRDiJn"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_question(model_input, max_length=32):\n",
        "    input_ids = MixQG_tokenizer(model_input, return_tensors=\"pt\", padding = True).input_ids\n",
        "    generated_ids = MixQG_model.generate(input_ids, max_length=32, num_beams=4) \n",
        "    questions = MixQG_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "    questions = process_questions(questions)\n",
        "    return questions"
      ],
      "metadata": {
        "id": "JiAlAZO_CnY7"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_questions(questions):\n",
        "    questions_m = []\n",
        "    for question in questions:\n",
        "        if question[-1] != '?':\n",
        "            question = question[0].upper() + question[1:] + '?'\n",
        "        questions_m.append(question)\n",
        "    return questions_m"
      ],
      "metadata": {
        "id": "LahSsrzcC3cr"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_type_inputs(answers_sentences, question_type):\n",
        "    model_input = []\n",
        "    if question_type == \"Wh-question\":\n",
        "        for pair in answers_sentences:\n",
        "            model_input.append(f\"{pair[0]} \\\\n {pair[1]}\")\n",
        "    elif question_type == \"TF-question\":\n",
        "        for pair in answers_sentences:\n",
        "            model_input.append(f\"{'yes'} {pair[0]} \\\\n {pair[1]}\")\n",
        "    elif question_type == \"any\":\n",
        "        for pair in answers_sentences:\n",
        "            model_input.append(f\"{pair[0]} \\\\n {pair[1]}\")\n",
        "            model_input.append(f\"{'yes'} {pair[0]} \\\\n {pair[1]}\")\n",
        "    return model_input"
      ],
      "metadata": {
        "id": "8ZcQRBiA6W7D"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Example"
      ],
      "metadata": {
        "id": "kB_igkOo4xNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/drive/My Drive/11611/Project/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pagGg7iiCMVV",
        "outputId": "e97b81bc-a487-466b-c65c-24bfa81c4179"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/11611/Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#这是要进行测试的文本\n",
        "with open('long_text.txt', 'r') as file:\n",
        "    context = file.read()"
      ],
      "metadata": {
        "id": "xs5WFPkU90cS"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(context.split()), \"words in this passage!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0A3HlivKzIW",
        "outputId": "78c7a53d-93a9-4ad6-9af7-b7218e9c7fa9"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4645 words in this passage!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Get_Questions_Main是主函数，N是生成问题的个数，在question_type和question_content都为any模式下，优先选Answer-agnositc模型生成的问题，如果不够会用answer-awarer模型的问题补充\n",
        "#2. 若question_type和question_content任意不为any，则只会调用answer-awarer模型，且N失效，产生多少个问题由能挖掘到多少相关答案决定（如一篇文章只有2个地方出现数字，问不出10个数字问题）\n",
        "#3. question_type可以是Wh-question, TF-question or any， question_content可以是entity, event, number or any。双any就是project要求的默认模式。\n",
        "#4. 函数既打印问题也返回问题列表\n",
        "generated = Get_Questions_Main(context, N = 50, question_type = 'any', question_content = 'any')"
      ],
      "metadata": {
        "id": "RMIxcY4_697y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14fd9ab8-89a8-4fb9-c472-e70d206cf124"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer agnostic model consumes:  47.124  seconds!\n",
            "Answer aware model consumes:  70.157  seconds!\n",
            "\n",
            "\n",
            "****************************Questions****************************\n",
            "What was Dempsey's first international tournament?\n",
            "When did Dempsey make his first appearance with the senior team?\n",
            "When did Dempsey join Tottenham Hotspur on a three-year contract?\n",
            "How many Premier League goals did Dempsey score in?\n",
            "What national team does Clint Dempsey play for?\n",
            "How many of Dempsey's passions are hip hop?\n",
            "In what league did Dempsey score a hat-trick?\n",
            "Where did Dempsey grow up?\n",
            "Who lived in a trailer park for most of Dempsey's childhood?\n",
            "Did dempsey play for the usa on November 17?\n",
            "What country did Dempsey play against in 2004?\n",
            "When did Dempsey score a hat-trick in the English Premier League?\n",
            "Is clint dempsey the captain of the sounders?\n",
            "Did clint play soccer on the sidelines?\n",
            "What team did Dempsey play for at Furman University?\n",
            "Did dempsey play in the starting lineup in the mls?\n",
            "Is clint dempsey born on march 9 1983?\n",
            "Did dallas texans play for furman university?\n",
            "Is clint dempsey a professional soccer player?\n",
            "Is there a youth soccer club in texas?\n",
            "Is dempsey the highest goal scorer in premier league?\n",
            "Does dempsey have a father on his side?\n",
            "Did early life and career dempsey grow up in texas?\n",
            "Did dempsey score a hat trick in January 2012?\n",
            "Did dempsey and his siblings play soccer with Hispanic immigrants?\n",
            "Was dempsey drafted by the new england revolution?\n",
            "When did Dempsey play for Fulham?\n",
            "When did Dempsey make his first appearance with the senior team?\n",
            "Did the dempseys ever go to the texans?\n",
            "Is dallas texans a top youth soccer club?\n",
            "Who is the first American male to score in three World Cups?\n",
            "What New England team did he play for?\n",
            "Are his parents in the texans with the dempseys?\n",
            "Where did Dempsey's family live during his childhood?\n",
            "How many times did he play for the Revolution?\n",
            "Does clint dempsey play for the seattle sounders?\n",
            "Is there a college soccer team in texas?\n",
            "When did Dempsey play in the adult league?\n",
            "Has an american player ever scored a hat-trick in the premier league?\n",
            "What league did Dempsey play for between 2007 and 2012?\n",
            "How long has he played for Fulham and Tottenham Hotspur?\n",
            "Did dempsey play soccer in his childhood?\n",
            "What did Dempsey have to do because of his family's financial constraints?\n",
            "Has he scored 25 goals in his career?\n",
            "Did dempsey play for premier league team  Fulham?\n",
            "How many goals did he score in 71 appearances for the Revolution?\n",
            "Did dempsey play soccer before playing for furman?\n",
            "Is the captain of the national team a soccer player?\n",
            "Did dempsey and his siblings play soccer together?\n",
            "What rank does he have in all time?\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5390dd7760e04a81aa8ca1416e87c4aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e505ded6ced54b42b439a404a43e75d4",
              "IPY_MODEL_f19274274da14b7280f6001464da70f4",
              "IPY_MODEL_a23bb532a3f5441da1592ce5d356c459"
            ],
            "layout": "IPY_MODEL_49e509e7eeed4325856f49a0a53e0404"
          }
        },
        "e505ded6ced54b42b439a404a43e75d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d88d864cf19846ca9f3b945913d0e0db",
            "placeholder": "​",
            "style": "IPY_MODEL_d9d602e5cca7419888416929a970d598",
            "value": "100%"
          }
        },
        "f19274274da14b7280f6001464da70f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbebe606b829463ba0d9f6e1d41c54c7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_031a5825bb784e898c3513c6113da26c",
            "value": 2
          }
        },
        "a23bb532a3f5441da1592ce5d356c459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d621877054eb4836aa16f34b2284d25e",
            "placeholder": "​",
            "style": "IPY_MODEL_81f88c3e296e40dfb5f865b549c99299",
            "value": " 2/2 [00:00&lt;00:00,  9.00it/s]"
          }
        },
        "49e509e7eeed4325856f49a0a53e0404": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d88d864cf19846ca9f3b945913d0e0db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9d602e5cca7419888416929a970d598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbebe606b829463ba0d9f6e1d41c54c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "031a5825bb784e898c3513c6113da26c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d621877054eb4836aa16f34b2284d25e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81f88c3e296e40dfb5f865b549c99299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}